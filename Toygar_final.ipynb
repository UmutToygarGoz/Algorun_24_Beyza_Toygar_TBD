{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu maalesef çalışmıyor !!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İlk olarak Toygar_Beyza_v.4.ipynb defterini çalıştırın"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class cfg():\n",
    "    data_path = \"/Users/user/Desktop/Algorun_24/data\"\n",
    "\n",
    "    promotion_calculation = \"/Users/user/Desktop/Algorun_24/Algorun_24_programs/Algorun_24_Beyza_Toygar/promotion_sales_calculations.csv\"\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    holidays_path = os.path.join(data_path, \"holidays.csv\")\n",
    "    products_path = os.path.join(data_path, \"products.csv\")\n",
    "    promotions_path = os.path.join(data_path, \"promotions.csv\")\n",
    "    sample_submission_path = os.path.join(data_path, \"sample_submission.csv\")\n",
    "    test_path = os.path.join(data_path, \"test.csv\")\n",
    "    train_path = os.path.join(data_path, \"train.csv\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "holidays_df = pd.read_csv(cfg.holidays_path)\n",
    "products_df = pd.read_csv(cfg.products_path)\n",
    "promotions_df = pd.read_csv(cfg.promotions_path)\n",
    "sample_submission_df = pd.read_csv(cfg.sample_submission_path, delimiter='|')\n",
    "test_df = pd.read_csv(cfg.test_path)\n",
    "train_df = pd.read_csv(cfg.train_path)\n",
    "\n",
    "# Convert columns to datetime format\n",
    "\n",
    "train_df[\"week_starting_date\"] = pd.to_datetime(train_df[\"week_starting_date\"])\n",
    "test_df[\"week_starting_date\"] = pd.to_datetime(test_df[\"week_starting_date\"])\n",
    "sample_submission_df[\"week_starting_date\"] = pd.to_datetime(sample_submission_df[\"week_starting_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"week_starting_date\"].max()\n",
    "\n",
    "train_df = train_df.merge(products_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "last_train_values_df = train_df[train_df[\"week_starting_date\"] == train_df[\"week_starting_date\"].max()]\n",
    "last_train_values_df = last_train_values_df[[\"product_id\", \"sales_quantity\"]]\n",
    "\n",
    "second_submission_df = sample_submission_df.copy()\n",
    "\n",
    "second_submission_df = second_submission_df.merge(last_train_values_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "second_submission_df[\"prediction\"] = second_submission_df[\"sales_quantity\"]\n",
    "\n",
    "second_submission_df = second_submission_df[[\"product_id\", \"week_starting_date\", \"prediction\"]]\n",
    "\n",
    "# Making negative numbers in prediction equal to 0\n",
    "second_submission_df.loc[second_submission_df[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "\n",
    "second_submission_df.fillna(0,inplace=True)\n",
    "\n",
    "second_submission_df.to_csv('second_submission.csv', index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/58/pms80m657fs8zklh9996g1br0000gp/T/ipykernel_61053/2377099430.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  last_2months_df['product_group_1_2_code'] = last_2months_df['product_group_1_code'].astype(str) + '_' + last_2months_df['product_group_2_code'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Fourth submission based on the sale of product groups(group_1 and group_2) during last year same_time, and ratio of sale of products in the last 2 months\n",
    "\n",
    "# Create a copy of the training data with shifted dates\n",
    "shifted_train_df = train_df.copy()\n",
    "shifted_train_df[\"week_starting_date\"] = train_df[\"week_starting_date\"] + dt.timedelta(days=371)\n",
    "\n",
    "# Combine product group codes into a single string column\n",
    "shifted_train_df['product_group_1_2_code'] = shifted_train_df['product_group_1_code'].astype(str) + '_' + shifted_train_df['product_group_2_code'].astype(str)\n",
    "\n",
    "important_dates1 = list(test_df[\"week_starting_date\"].unique())\n",
    "prepared_shifted_train_df = shifted_train_df[\n",
    "    shifted_train_df[\"week_starting_date\"].isin(important_dates1)\n",
    "][[\"week_starting_date\", \"product_id\", \"sales_quantity\", \"product_group_1_2_code\"]]\n",
    "\n",
    "# Group by week_starting_date and combined product group code\n",
    "grouped_sales = prepared_shifted_train_df.groupby(\n",
    "    ['week_starting_date', 'product_group_1_2_code'])['sales_quantity'].sum().reset_index()\n",
    "\n",
    "# Calculate ratios for the last 2 months\n",
    "last_date = train_df['week_starting_date'].max()\n",
    "two_months_ago = last_date - pd.DateOffset(months=2)\n",
    "last_2months_df = train_df[train_df['week_starting_date'] >= two_months_ago]\n",
    "\n",
    "# Add the combined product group code to the last_2months_df\n",
    "last_2months_df['product_group_1_2_code'] = last_2months_df['product_group_1_code'].astype(str) + '_' + last_2months_df['product_group_2_code'].astype(str)\n",
    "\n",
    "# Calculate ratios with the combined group code\n",
    "product_sales = last_2months_df.groupby('product_id')['sales_quantity'].sum()\n",
    "group_sales = last_2months_df.groupby('product_group_1_2_code')['sales_quantity'].sum()\n",
    "\n",
    "sales_ratio = (\n",
    "    last_2months_df[['product_id', 'product_group_1_2_code']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('product_id')\n",
    ")\n",
    "\n",
    "sales_ratio['product_sales'] = product_sales\n",
    "sales_ratio['group_total_sales'] = sales_ratio['product_group_1_2_code'].map(group_sales)\n",
    "sales_ratio['sales_ratio'] = sales_ratio['product_sales'] / sales_ratio['group_total_sales']\n",
    "sales_ratio = sales_ratio.reset_index()\n",
    "\n",
    "# Merge the data using the combined product group code\n",
    "merged_data = pd.merge(\n",
    "    grouped_sales[['week_starting_date', 'product_group_1_2_code', 'sales_quantity']],\n",
    "    sales_ratio[['product_group_1_2_code', 'product_id', 'sales_ratio']],\n",
    "    on='product_group_1_2_code'\n",
    ")\n",
    "\n",
    "# Calculate predicted sales\n",
    "merged_data['predicted_sales'] = merged_data['sales_quantity'] * merged_data['sales_ratio']\n",
    "\n",
    "# Create final predictions dataframe\n",
    "predictions_df = merged_data[['week_starting_date', 'product_id', \n",
    "                            'product_group_1_2_code', 'predicted_sales']]\n",
    "\n",
    "\n",
    "fourth_submission_df = sample_submission_df.copy()\n",
    "\n",
    "fourth_submission_df = fourth_submission_df.merge(predictions_df, on=[\"product_id\",\"week_starting_date\"], how=\"left\")\n",
    "\n",
    "fourth_submission_df[\"prediction\"] = fourth_submission_df[\"predicted_sales\"]\n",
    "\n",
    "fourth_submission_df = fourth_submission_df[[\"product_id\", \"week_starting_date\", \"prediction\"]]\n",
    "\n",
    "# Making negative numbers in prediction equal to 0\n",
    "fourth_submission_df.loc[fourth_submission_df[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "\n",
    "fourth_submission_df.fillna(0,inplace=True)\n",
    "\n",
    "fourth_submission_df.to_csv('fourth_submission.csv', index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'total_sales_quantity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_sales_quantity'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m past_promotions_df \u001b[38;5;241m=\u001b[39m promotions_df[promotions_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_sales_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m promotion_stats \u001b[38;5;241m=\u001b[39m past_promotions_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpromotion_type\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_sales_quantity_per_day\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_sales_quantity\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m })\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_test_sales\u001b[39m(test_df, promotions_df, promotion_stats):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Print diagnostic information\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_sales_quantity'"
     ]
    }
   ],
   "source": [
    "\n",
    "past_promotions_df = promotions_df[promotions_df[\"total_sales_quantity\"]!=0]\n",
    "\n",
    "promotion_stats = past_promotions_df.groupby('promotion_type').agg({\n",
    "    'mean_sales_quantity_per_day': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'total_sales_quantity': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_test_sales(test_df, promotions_df, promotion_stats):\n",
    "    # Print diagnostic information\n",
    "    print(\"Test weeks:\")\n",
    "    print(test_df['week_starting_date'].unique())\n",
    "    \n",
    "    print(\"\\nPromotions during test period:\")\n",
    "    mask = (promotions_df['start_date'] <= test_df['week_starting_date'].max() + pd.Timedelta(days=6)) & \\\n",
    "           (promotions_df['end_date'] >= test_df['week_starting_date'].min())\n",
    "    print(promotions_df[mask][['promotion_type', 'start_date', 'end_date']])\n",
    "    \n",
    "    print(\"\\nPromotion statistics:\")\n",
    "    print(promotion_stats)\n",
    "    \n",
    "    # Get unique week starting dates\n",
    "    result_df = pd.DataFrame({'week_starting_date': test_df['week_starting_date'].unique()})\n",
    "    result_df = result_df.sort_values('week_starting_date').reset_index(drop=True)\n",
    "    result_df['week_end_date'] = result_df['week_starting_date'] + pd.Timedelta(days=6)\n",
    "    result_df['predicted_sales'] = 0.0\n",
    "    \n",
    "    # Get mean daily sales for each promotion type\n",
    "    mean_daily_sales = promotion_stats['mean_sales_quantity_per_day']['mean']\n",
    "    \n",
    "    # For each week, calculate the sales based on active promotions\n",
    "    for idx, row in result_df.iterrows():\n",
    "        week_start = row['week_starting_date']\n",
    "        week_end = row['week_end_date']\n",
    "        \n",
    "        # Find promotions active during this week\n",
    "        active_promos = promotions_df[\n",
    "            (promotions_df['start_date'] <= week_end) & \n",
    "            (promotions_df['end_date'] >= week_start)\n",
    "        ]\n",
    "        \n",
    "        if len(active_promos) > 0:\n",
    "            for _, promo in active_promos.iterrows():\n",
    "                # Calculate overlap days\n",
    "                overlap_start = max(week_start, promo['start_date'])\n",
    "                overlap_end = min(week_end, promo['end_date'])\n",
    "                overlap_days = (overlap_end - overlap_start).days + 1\n",
    "                \n",
    "                # Add sales for this promotion\n",
    "                daily_sales = mean_daily_sales[promo['promotion_type']]\n",
    "                result_df.loc[idx, 'predicted_sales'] += daily_sales * overlap_days\n",
    "            \n",
    "            # If multiple promotions, take average\n",
    "            if len(active_promos) > 1:\n",
    "                result_df.loc[idx, 'predicted_sales'] /= len(active_promos)\n",
    "    \n",
    "    return result_df[['week_starting_date', 'predicted_sales']]\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_test_sales(test_df, promotions_df, promotion_stats)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFinal Predictions for test weeks:\")\n",
    "print(predictions)\n",
    "\n",
    "# First calculate the ratios as before\n",
    "last_date = train_df['week_starting_date'].max()\n",
    "two_months_ago = last_date - pd.DateOffset(months=2)\n",
    "last_2months_df = train_df[train_df['week_starting_date'] >= two_months_ago]\n",
    "\n",
    "# Calculate ratios\n",
    "product_sales = last_2months_df.groupby('product_id')['sales_quantity'].sum()\n",
    "total_sales = last_2months_df['sales_quantity'].sum()  # Changed to total sales\n",
    "\n",
    "sales_ratio = (\n",
    "    last_2months_df[['product_id']]  # Removed product_group_1_code since we don't need it\n",
    "    .drop_duplicates()\n",
    "    .set_index('product_id')\n",
    ")\n",
    "\n",
    "sales_ratio['product_sales'] = product_sales\n",
    "sales_ratio['total_sales'] = total_sales  # Changed to total sales\n",
    "sales_ratio['sales_ratio'] = sales_ratio['product_sales'] / sales_ratio['total_sales']\n",
    "sales_ratio = sales_ratio.reset_index()\n",
    "\n",
    "# Create a cartesian product (cross join) using merge with dummy key\n",
    "predictions['key'] = 1\n",
    "sales_ratio['key'] = 1\n",
    "merged_data = pd.merge(\n",
    "    predictions[['week_starting_date', 'predicted_sales', 'key']],\n",
    "    sales_ratio[['product_id', 'sales_ratio', 'key']],\n",
    "    on='key'\n",
    ").drop('key', axis=1)\n",
    "\n",
    "# Vectorized calculation of predicted sales per product\n",
    "merged_data['predicted_product_sales'] = merged_data['predicted_sales'] * merged_data['sales_ratio']\n",
    "\n",
    "# Select required columns for final output\n",
    "predictions_df = merged_data[['week_starting_date', 'product_id', 'predicted_product_sales']]\n",
    "\n",
    "seventh_submission_df = sample_submission_df.copy()\n",
    "\n",
    "seventh_submission_df = seventh_submission_df.merge(predictions_df, on=[\"product_id\",\"week_starting_date\"], how=\"left\")\n",
    "\n",
    "seventh_submission_df[\"prediction\"] = seventh_submission_df[\"predicted_product_sales\"]\n",
    "\n",
    "seventh_submission_df = seventh_submission_df[[\"product_id\", \"week_starting_date\", \"prediction\"]]\n",
    "\n",
    "# Making negative numbers in prediction equal to 0\n",
    "seventh_submission_df.loc[seventh_submission_df[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "\n",
    "seventh_submission_df.fillna(0,inplace=True)\n",
    "\n",
    "seventh_submission_df.to_csv('seventh_submission.csv', index=False,sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['product_group_1_2_3_code'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m shifted_train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek_starting_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek_starting_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m dt\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m371\u001b[39m)\n\u001b[1;32m      8\u001b[0m important_dates1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek_starting_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m----> 9\u001b[0m prepared_shifted_train_df \u001b[38;5;241m=\u001b[39m shifted_train_df[\n\u001b[1;32m     10\u001b[0m     shifted_train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek_starting_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(important_dates1)\n\u001b[1;32m     11\u001b[0m ][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek_starting_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_group_1_2_3_code\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Group by week_starting_date and product_group_1_2_3_code and sum the sales_quantity\u001b[39;00m\n\u001b[1;32m     14\u001b[0m grouped_sales \u001b[38;5;241m=\u001b[39m prepared_shifted_train_df\u001b[38;5;241m.\u001b[39mgroupby(\n\u001b[1;32m     15\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek_starting_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_group_1_2_3_code\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales_quantity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['product_group_1_2_3_code'] not in index\""
     ]
    }
   ],
   "source": [
    "# Thirteenth submission based on the sale of product groups(group_1_2_3) during last year same_time, and ratio of sale of products in the last 2 months\n",
    "\n",
    "# Assuming your DataFrame 'df' has columns: 'product_id', 'product_group', 'sale_date', 'sale_amount'\n",
    "\n",
    "shifted_train_df = train_df.copy()\n",
    "shifted_train_df[\"week_starting_date\"] = train_df[\"week_starting_date\"] + dt.timedelta(days=371)\n",
    "\n",
    "important_dates1 = list(test_df[\"week_starting_date\"].unique())\n",
    "prepared_shifted_train_df = shifted_train_df[\n",
    "    shifted_train_df[\"week_starting_date\"].isin(important_dates1)\n",
    "][[\"week_starting_date\", \"product_id\", \"sales_quantity\", \"product_group_1_2_3_code\"]]\n",
    "\n",
    "# Group by week_starting_date and product_group_1_2_3_code and sum the sales_quantity\n",
    "grouped_sales = prepared_shifted_train_df.groupby(\n",
    "    ['week_starting_date', 'product_group_1_2_3_code'])['sales_quantity'].sum().reset_index()\n",
    "\n",
    "# If you want to reshape the data to have product groups as columns\n",
    "# pivoted_sales = grouped_sales.pivot(\n",
    "#     index='week_starting_date', \n",
    "#     columns='product_group_1_2_3_code', \n",
    "#     values='sales_quantity'\n",
    "# )\n",
    "\n",
    "\n",
    "# First calculate the ratios as before\n",
    "last_date = train_df['week_starting_date'].max()\n",
    "two_months_ago = last_date - pd.DateOffset(months=2)\n",
    "last_2months_df = train_df[train_df['week_starting_date'] >= two_months_ago]\n",
    "\n",
    "# Calculate ratios\n",
    "product_sales = last_2months_df.groupby('product_id')['sales_quantity'].sum()\n",
    "group_sales = last_2months_df.groupby('product_group_1_2_3_code')['sales_quantity'].sum()\n",
    "\n",
    "sales_ratio = (\n",
    "    last_2months_df[['product_id', 'product_group_1_2_3_code']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('product_id')\n",
    ")\n",
    "\n",
    "sales_ratio['product_sales'] = product_sales\n",
    "sales_ratio['group_total_sales'] = sales_ratio['product_group_1_2_3_code'].map(group_sales)\n",
    "sales_ratio['sales_ratio'] = sales_ratio['product_sales'] / sales_ratio['group_total_sales']\n",
    "sales_ratio = sales_ratio.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Perform a merge operation instead of iterating\n",
    "merged_data = pd.merge(\n",
    "    grouped_sales[['week_starting_date', 'product_group_1_2_3_code', 'sales_quantity']],\n",
    "    sales_ratio[['product_group_1_2_3_code', 'product_id', 'sales_ratio']],\n",
    "    on='product_group_1_2_3_code'\n",
    ")\n",
    "\n",
    "# Vectorized calculation of predicted sales\n",
    "merged_data['predicted_sales'] = merged_data['sales_quantity'] * merged_data['sales_ratio']\n",
    "\n",
    "# Select required columns for final output\n",
    "predictions_df = merged_data[['week_starting_date', 'product_id', 'predicted_sales']]\n",
    "\n",
    "thirteenth_submission_df = sample_submission_df.copy()\n",
    "\n",
    "thirteenth_submission_df = thirteenth_submission_df.merge(predictions_df, on=[\"product_id\",\"week_starting_date\"], how=\"left\")\n",
    "\n",
    "thirteenth_submission_df[\"prediction\"] = thirteenth_submission_df[\"predicted_sales\"]\n",
    "\n",
    "thirteenth_submission_df = thirteenth_submission_df[[\"product_id\", \"week_starting_date\", \"prediction\"]]\n",
    "\n",
    "# Making negative numbers in prediction equal to 0\n",
    "thirteenth_submission_df.loc[thirteenth_submission_df[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "\n",
    "\n",
    "thirteenth_submission_df = sample_submission_df.copy()\n",
    "\n",
    "# First merge with predictions\n",
    "thirteenth_submission_df = thirteenth_submission_df.merge(predictions_df, on=[\"product_id\",\"week_starting_date\"], how=\"left\")\n",
    "\n",
    "# Merge with product information to get product_group_1_2_3_code\n",
    "thirteenth_submission_df = thirteenth_submission_df.merge(\n",
    "    train_df[['product_id', 'product_group_1_2_3_code']].drop_duplicates(),\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN values with mean predictions for the same product group and week\n",
    "thirteenth_submission_df['predicted_sales'] = thirteenth_submission_df.groupby(\n",
    "    ['product_group_1_2_3_code', 'week_starting_date'])['predicted_sales'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "\n",
    "thirteenth_submission_df[\"prediction\"] = thirteenth_submission_df[\"predicted_sales\"]\n",
    "\n",
    "\n",
    "\n",
    "thirteenth_submission_df.fillna(3.5, inplace=True)\n",
    "\n",
    "\n",
    "# Keep only required columns\n",
    "thirteenth_submission_df = thirteenth_submission_df[[\"product_id\", \"week_starting_date\", \"prediction\"]]\n",
    "\n",
    "# Making negative numbers in prediction equal to 0\n",
    "thirteenth_submission_df.loc[thirteenth_submission_df[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "\n",
    "thirteenth_submission_df.isna().sum()\n",
    "\n",
    "thirteenth_submission_df.to_csv('thirteenth_submission.csv', index=False,sep=\"|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourteenth submission will be based on thirteenth submission, but the predictions will be multiplied with (thirteenth submission total sales + promotion calculation total sales)/ (2*thirteenth submission total sales)\n",
    "\n",
    "# Read thirteenth submission\n",
    "thirteenth_submission_df = pd.read_csv('thirteenth_submission.csv', delimiter='|')\n",
    "thirteenth_submission_df['week_starting_date'] = pd.to_datetime(thirteenth_submission_df['week_starting_date'])\n",
    "\n",
    "# Calculate total sales from thirteenth submission\n",
    "thirteenth_total_sales = thirteenth_submission_df['prediction'].sum()\n",
    "\n",
    "# Calculate promotion-based predictions\n",
    "def predict_test_sales(test_df, promotions_df, promotion_stats):\n",
    "    result_df = pd.DataFrame({'week_starting_date': test_df['week_starting_date'].unique()})\n",
    "    result_df = result_df.sort_values('week_starting_date').reset_index(drop=True)\n",
    "    result_df['week_end_date'] = result_df['week_starting_date'] + pd.Timedelta(days=6)\n",
    "    result_df['predicted_sales'] = 0.0\n",
    "    \n",
    "    mean_daily_sales = promotion_stats['mean_sales_quantity_per_day']['mean']\n",
    "    \n",
    "    for idx, row in result_df.iterrows():\n",
    "        week_start = row['week_starting_date']\n",
    "        week_end = row['week_end_date']\n",
    "        \n",
    "        active_promos = promotions_df[\n",
    "            (promotions_df['start_date'] <= week_end) & \n",
    "            (promotions_df['end_date'] >= week_start)\n",
    "        ]\n",
    "        \n",
    "        if len(active_promos) > 0:\n",
    "            for _, promo in active_promos.iterrows():\n",
    "                overlap_start = max(week_start, promo['start_date'])\n",
    "                overlap_end = min(week_end, promo['end_date'])\n",
    "                overlap_days = (overlap_end - overlap_start).days + 1\n",
    "                \n",
    "                daily_sales = mean_daily_sales[promo['promotion_type']]\n",
    "                result_df.loc[idx, 'predicted_sales'] += daily_sales * overlap_days\n",
    "            \n",
    "            if len(active_promos) > 1:\n",
    "                result_df.loc[idx, 'predicted_sales'] /= len(active_promos)\n",
    "    \n",
    "    return result_df[['week_starting_date', 'predicted_sales']]\n",
    "\n",
    "# Generate promotion-based predictions\n",
    "promotion_predictions = pd.read_csv(cfg.promotion_calculation,delimiter=\"|\")\n",
    "promotion_total_sales = promotion_predictions['prediction'].sum()\n",
    "\n",
    "# Calculate adjustment factor\n",
    "adjustment_factor = (thirteenth_total_sales + promotion_total_sales) / (2 * thirteenth_total_sales)\n",
    "\n",
    "# Create fourteenth submission\n",
    "fourteenth_submission_df = thirteenth_submission_df.copy()\n",
    "fourteenth_submission_df['prediction'] = fourteenth_submission_df['prediction'] * adjustment_factor\n",
    "\n",
    "# Ensure no negative predictions\n",
    "fourteenth_submission_df.loc[fourteenth_submission_df['prediction'] < 0, 'prediction'] = 0\n",
    "\n",
    "# Round predictions to desired decimal places (optional)\n",
    "fourteenth_submission_df['prediction'] = fourteenth_submission_df['prediction'].round(2)\n",
    "\n",
    "# Save the results\n",
    "fourteenth_submission_df.to_csv('fourteenth_submission.csv', sep='|', index=False)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Thirteenth submission total sales: {thirteenth_total_sales:,.2f}\")\n",
    "print(f\"Promotion-based total sales: {promotion_total_sales:,.2f}\")\n",
    "print(f\"Adjustment factor: {adjustment_factor:.4f}\")\n",
    "print(f\"Fourteenth submission total sales: {fourteenth_submission_df['prediction'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifteenth submission is based on eighth submission, but instead of filling NaN with 0, I will be filling with the value that minimizes RMSLE in train_df\n",
    "\n",
    "import pandas as pd\n",
    "second_submission_df = pd.read_csv('second_submission.csv',delimiter=\"|\")\n",
    "fourth_submission_df = pd.read_csv('fourth_submission.csv',delimiter=\"|\")\n",
    "fourteenth_submission_df = pd.read_csv('fourteenth_submission.csv', delimiter=\"|\")\n",
    "\n",
    "eighth_submission_df2 = second_submission_df.copy()\n",
    "eighth_submission_df2[\"prediction\"] = (8 * second_submission_df[\"prediction\"] + fourth_submission_df[\"prediction\"] + fourteenth_submission_df[\"prediction\"]) / 10\n",
    "\n",
    "# Making negative numbers in prediction equal to 0\n",
    "eighth_submission_df2.loc[eighth_submission_df2[\"prediction\"] < 0, \"prediction\"] = 0\n",
    "\n",
    "eighth_submission_df2.fillna(0,inplace=True)\n",
    "\n",
    "eighth_submission_df2.to_csv('eighth_submission.csv', index=False, sep=\"|\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fifteenth_submission_df = eighth_submission_df2.copy()\n",
    "fifteenth_submission_df['prediction'] = fifteenth_submission_df['prediction'].fillna(0)\n",
    "\n",
    "# Ensure no negative predictions\n",
    "fifteenth_submission_df.loc[fifteenth_submission_df['prediction'] < 0, 'prediction'] = 0\n",
    "\n",
    "# Keep only required columns\n",
    "fifteenth_submission_df = fifteenth_submission_df[['product_id', 'week_starting_date', 'prediction']]\n",
    "\n",
    "# Save the results\n",
    "fifteenth_submission_df.to_csv('fifteenth_submission.csv', sep='|', index=False)\n",
    "\n",
    "fifteenth_submission_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sixteenth submission is based on a new ensemble\n",
    "\n",
    "second_submission_df = pd.read_csv('second_submission.csv',delimiter=\"|\")\n",
    "fourth_submission_df = pd.read_csv('fourth_submission.csv',delimiter=\"|\")\n",
    "seventh_submission_df = pd.read_csv('seventh_submission.csv', delimiter=\"|\")\n",
    "fourteenth_submission_df = pd.read_csv('fourteenth_submission.csv', delimiter=\"|\")\n",
    "\n",
    "\n",
    "sixteenth_submission_df = sample_submission_df.copy()\n",
    "sixteenth_submission_df[\"prediction\"] = (2 * second_submission_df[\"prediction\"] + fourth_submission_df[\"prediction\"] + seventh_submission_df[\"prediction\"] + fourteenth_submission_df[\"prediction\"]) / 5\n",
    "sixteenth_submission_df['prediction'] = sixteenth_submission_df['prediction'].fillna(0)\n",
    "\n",
    "# Ensure no negative predictions\n",
    "sixteenth_submission_df.loc[sixteenth_submission_df['prediction'] < 0, 'prediction'] = 0\n",
    "\n",
    "sixteenth_submission_df['prediction'] = sixteenth_submission_df['prediction']*(16.947841/17.474522)\n",
    "\n",
    "# Keep only required columns\n",
    "sixteenth_submission_df = sixteenth_submission_df[['product_id', 'week_starting_date', 'prediction']]\n",
    "\n",
    "# Save the results\n",
    "sixteenth_submission_df.to_csv('sixteenth_submission.csv', sep='|', index=False)\n",
    "\n",
    "sixteenth_submission_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
